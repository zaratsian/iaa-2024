{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SWE-YUBsNthe"
      },
      "source": [
        "## **Spark ML Assignment**\n",
        "The goal of this assignment is to (1) use Spark to analyze and process data and (2) to train a Spark ML Model.\n",
        "\n",
        "I've provided this template as a potentially helpful framework, but feel free to use other data processing and ML algorithms based on your dataset and on your personal preferences. \n",
        "\n",
        "<br>\n",
        "\n",
        "**Assignment Tasks:**\n",
        "\n",
        "1) Load the CSV into a Spark Dataframe. I've added code to automatically download \"banking_attrition.csv\" for you so that you have it locally, however, if you prefer to use another dataset for your analysis, then feel free to use that instead. \n",
        "\n",
        "2) Preprocess the data, applying at least one data exploration or cleaning techniques.\n",
        "\n",
        "3) Split the historical data into train and test.\n",
        "\n",
        "4) Train the model (Please predict \"attrition\" probability if you use the banking_attrition.csv\" dataset). \n",
        "\n",
        "5) Test the model performance against your test dataset.\n",
        "\n",
        "6) Display model evaluation / model fit statistics so that I can see the performance. For this assignment, I'm not focused too much on getting a good model performance, but rather I'm evaluating your process and the spark code that you write in order to prepocess and train the model.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tBqp2EKl2mhX"
      },
      "source": [
        "## **Install PySpark Dependencies**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qXWkpWwR2gVR"
      },
      "source": [
        "# Install Spark dependencies\n",
        "!apt-get install openjdk-8-jdk-headless -qq > /dev/null\n",
        "!rm spark-3.2.3-bin-hadoop3.2.tgz\n",
        "!wget --no-cookies --no-check-certificate https://dlcdn.apache.org/spark/spark-3.2.3/spark-3.2.3-bin-hadoop3.2.tgz\n",
        "!tar zxvf spark-3.2.3-bin-hadoop3.2.tgz\n",
        "!pip install -q findspark\n",
        "!pip install pyspark==3.2.3"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BEciKx-M2y75"
      },
      "source": [
        "## **Load Data**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iYrbIewX23lA"
      },
      "source": [
        "!wget https://raw.githubusercontent.com/zaratsian/Datasets/master/banking_attrition.csv"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8BpC4MLS25kj"
      },
      "source": [
        "## **Import Python / Spark Libraries**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lRZl0-Xt2_NE"
      },
      "source": [
        "import os\n",
        "os.environ[\"JAVA_HOME\"]  = \"/usr/lib/jvm/java-8-openjdk-amd64\"\n",
        "os.environ[\"SPARK_HOME\"] = \"/content/spark-3.2.3-bin-hadoop3.2\"\n",
        "\n",
        "import datetime, time\n",
        "import re, random, sys\n",
        "\n",
        "# Note - Not all of these will be used, but I've added them for your reference as a \"getting started\"\n",
        "from pyspark import SparkContext\n",
        "from pyspark.sql import SparkSession\n",
        "from pyspark.sql.types import StructType, StructField, ArrayType, IntegerType, StringType, FloatType, LongType, DateType\n",
        "from pyspark.sql.functions import struct, array, lit, monotonically_increasing_id, col, expr, when, concat, udf, split, size, lag, count, isnull\n",
        "from pyspark.sql import Window\n",
        "from pyspark.ml.linalg import Vectors\n",
        "from pyspark.ml.regression import GBTRegressor, LinearRegression, GeneralizedLinearRegression, RandomForestRegressor\n",
        "from pyspark.ml.classification import GBTClassifier, RandomForestClassifier\n",
        "from pyspark.ml.feature import VectorIndexer, VectorAssembler, StringIndexer, IndexToString\n",
        "from pyspark.ml.evaluation import RegressionEvaluator\n",
        "from pyspark.ml import Pipeline, PipelineModel\n",
        "from pyspark.ml.evaluation import MulticlassClassificationEvaluator, RegressionEvaluator"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nvK0YRFx78RY"
      },
      "source": [
        "## **Create Spark Session**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dwGkL0GW7-_j"
      },
      "source": [
        "spark = SparkSession.builder.appName(\"Spark ML Assignment\").master(\"local[*]\").getOrCreate()"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gnbtnMcp4v3E"
      },
      "source": [
        "## **Load CSV Data into Spark Dataframe**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "062IYTp24zQh"
      },
      "source": [],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Wu8K-qCi2-Kk"
      },
      "source": [
        "## **Data Exploration**\n",
        "Perform at least one data exploration of your choice (This could be a basic show(), an aggregation/[groupby](https://spark.apache.org/docs/latest/api/python/pyspark.sql.html#pyspark.sql.DataFrame.groupBy), [correlation](https://spark.apache.org/docs/latest/ml-statistics.html#correlation), [summarizer](https://spark.apache.org/docs/latest/ml-statistics.html#summarizer), etc.)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_sBDC5db4FL-"
      },
      "source": [],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wkPpObFZ4Pax"
      },
      "source": [
        "## **Split the Spark Dataframe into Train and Test**\n",
        "You could use a [randomsplit](https://spark.apache.org/docs/latest/api/python/pyspark.sql.html#pyspark.sql.DataFrame.randomSplit) here, a [Cross Validator](https://spark.apache.org/docs/latest/ml-tuning.html#cross-validation), or another approach of your choice."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AmsQfLKA4WwE"
      },
      "source": [],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mvdCBX2D4WNJ"
      },
      "source": [
        "## **Feature Engineering**\n",
        "During this step, I'd like to see you convert at least one STRING variable (such as gender, membership, education or another variable of your choice) into a numeric representation so that you can use it as one of the model inputs. You can convert the string to a numeric by using [one-hot encoding](https://spark.apache.org/docs/latest/ml-features.html#onehotencoderestimator), a [stringindexer](https://spark.apache.org/docs/latest/ml-features.html#stringindexer), etc\n",
        "\n",
        "You will also want to define a ML model object. An example of this would be a random forest, gradient boosting, or some other approach listed [here](https://spark.apache.org/docs/latest/ml-classification-regression.html). "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HqAxflMW5IpU"
      },
      "source": [],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lwmmMYzP5MpE"
      },
      "source": [
        "## **Fit/Train ML Model**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a1igoWRu5Pz7"
      },
      "source": [],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0-TVO6s55h05"
      },
      "source": [
        "## **Make Predictions**\n",
        "Use your model to make predications against the Test (holdout) Dataframe"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RVuqPtEI5hnX"
      },
      "source": [],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lHkwh8mM5Si7"
      },
      "source": [
        "## **Evaluate Model against Test Dataframe**\n",
        "Display model fit statistics, such as RMSE or MSE"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ORJ8UOuH5wPX"
      },
      "source": [],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Save the Model Object (Optional)\n",
        "\n",
        "Write spark code that saves your model object. \n",
        "\n",
        "<br>\n",
        "\n",
        "Context: For production purposes, it's often requierd to save the model object so that it can be deployed as a stand-alone and compressed binary object. The model object is typically wrapped in a container and served as a REST or gRPC endpoint."
      ],
      "metadata": {
        "id": "8_a40JoZ7W7V"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "ZyyZqKkj7cIf"
      },
      "execution_count": 5,
      "outputs": []
    }
  ]
}